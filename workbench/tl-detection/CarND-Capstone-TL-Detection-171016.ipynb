{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND Capstone TL-Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# Let's get started!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageColor\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "def vanilla_conv_block(x, kernel_size, output_channels):\n",
    "    \"\"\"\n",
    "    Vanilla Conv -> Batch Norm -> ReLU\n",
    "    \"\"\"\n",
    "    x = tf.layers.conv2d(\n",
    "        x, output_channels, kernel_size, (2, 2), padding='SAME')\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# TODO: implement MobileNet conv block\n",
    "def mobilenet_conv_block(x, kernel_size, output_channels):\n",
    "    \"\"\"\n",
    "    Depthwise Conv -> Batch Norm -> ReLU -> Pointwise Conv -> Batch Norm -> ReLU\n",
    "    \"\"\"\n",
    "    # assumes BHWC format\n",
    "    input_channel_dim = x.get_shape().as_list()[-1] \n",
    "    W = tf.Variable(tf.truncated_normal((kernel_size, kernel_size, input_channel_dim, 1)))\n",
    "\n",
    "    # depthwise conv\n",
    "    x = tf.nn.depthwise_conv2d(x, W, (1, 2, 2, 1), padding='SAME')\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # pointwise conv\n",
    "    x = tf.layers.conv2d(x, output_channels, (1, 1), padding='SAME')\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VANILLA CONV BLOCK\n",
      "Variable vanilla/conv2d/kernel:0: number of params = 147456\n",
      "Variable vanilla/conv2d/bias:0: number of params = 512\n",
      "Variable vanilla/batch_normalization/beta:0: number of params = 512\n",
      "Variable vanilla/batch_normalization/gamma:0: number of params = 512\n",
      "Total number of params = 148992\n",
      "\n",
      "MOBILENET CONV BLOCK\n",
      "Variable mobile/Variable:0: number of params = 288\n",
      "Variable mobile/batch_normalization/beta:0: number of params = 32\n",
      "Variable mobile/batch_normalization/gamma:0: number of params = 32\n",
      "Variable mobile/conv2d/kernel:0: number of params = 16384\n",
      "Variable mobile/conv2d/bias:0: number of params = 512\n",
      "Variable mobile/batch_normalization_1/beta:0: number of params = 512\n",
      "Variable mobile/batch_normalization_1/gamma:0: number of params = 512\n",
      "Total number of params = 18272\n",
      "\n",
      "8.154x parameter reduction\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "# Compare the number of parameters in each block.\n",
    "# The solution should show the majority of the parameters in MobileNet block stem from the pointwise convolution.\n",
    "\n",
    "# constants but you can change them so I guess they're not so constant :)\n",
    "INPUT_CHANNELS = 32\n",
    "OUTPUT_CHANNELS = 512\n",
    "KERNEL_SIZE = 3\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # input\n",
    "    x = tf.constant(np.random.randn(1, IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS), dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('vanilla'):\n",
    "        vanilla_conv = vanilla_conv_block(x, KERNEL_SIZE, OUTPUT_CHANNELS)\n",
    "    with tf.variable_scope('mobile'):\n",
    "        mobilenet_conv = mobilenet_conv_block(x, KERNEL_SIZE, OUTPUT_CHANNELS)\n",
    "\n",
    "    vanilla_params = [\n",
    "        (v.name, np.prod(v.get_shape().as_list()))\n",
    "        for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'vanilla')\n",
    "    ]\n",
    "    mobile_params = [\n",
    "        (v.name, np.prod(v.get_shape().as_list()))\n",
    "        for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'mobile')\n",
    "    ]\n",
    "\n",
    "    print(\"VANILLA CONV BLOCK\")\n",
    "    total_vanilla_params = sum([p[1] for p in vanilla_params])\n",
    "    for p in vanilla_params:\n",
    "        print(\"Variable {0}: number of params = {1}\".format(p[0], p[1]))\n",
    "    print(\"Total number of params =\", total_vanilla_params)\n",
    "    print()\n",
    "\n",
    "    print(\"MOBILENET CONV BLOCK\")\n",
    "    total_mobile_params = sum([p[1] for p in mobile_params])\n",
    "    for p in mobile_params:\n",
    "        print(\"Variable {0}: number of params = {1}\".format(p[0], p[1]))\n",
    "    print(\"Total number of params =\", total_mobile_params)\n",
    "    print()\n",
    "\n",
    "    print(\"{0:.3f}x parameter reduction\".format(total_vanilla_params /\n",
    "                                             total_mobile_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "#SSD Summary\n",
    "\n",
    "#    Start from a pretrained base model.\n",
    "#    The base model is extended by several convolutional layers.\n",
    "#    Each feature map is used to predict bounding boxes. Diversity in feature map size allows object detection at different resolutions.\n",
    "#    Boxes are filtered by IoU metrics and hard negative mining.\n",
    "#    Loss is a combination of classification (softmax) and dectection (smooth L1)\n",
    "#    Model can be trained end to end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "## Object Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "# In this part we'll detect objects using pretrained object detection models. \n",
    "# Pretrained models can be downloaded from the model zoo.\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "\n",
    "# Frozen inference graph files. NOTE: change the path to where you saved the models.\n",
    "#SSD_GRAPH_FILE = 'ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "#RFCN_GRAPH_FILE = 'rfcn_resnet101_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "#SSD_INCEPTION_GRAPH_FILE = 'ssd_inception_v2_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "\n",
    "FASTER_RCNN_GRAPH_FILE = 'faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017/frozen_inference_graph.pb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of colors = 148\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "# Below are utility functions. \n",
    "# The main purpose of these is to draw the bounding boxes back onto the original image.\n",
    "\n",
    "\n",
    "# Colors (one for each class)\n",
    "cmap = ImageColor.colormap\n",
    "print(\"Number of colors =\", len(cmap))\n",
    "COLOR_LIST = sorted([c for c in cmap.keys()])\n",
    "\n",
    "#\n",
    "# Utility funcs\n",
    "#\n",
    "\n",
    "def filter_boxes(min_score, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def to_image_coords(boxes, height, width):\n",
    "    \"\"\"\n",
    "    The original box coordinate output is normalized, i.e [0, 1].\n",
    "    \n",
    "    This converts it back to the original coordinate based on the image\n",
    "    size.\n",
    "    \"\"\"\n",
    "    box_coords = np.zeros_like(boxes)\n",
    "    box_coords[:, 0] = boxes[:, 0] * height\n",
    "    box_coords[:, 1] = boxes[:, 1] * width\n",
    "    box_coords[:, 2] = boxes[:, 2] * height\n",
    "    box_coords[:, 3] = boxes[:, 3] * width\n",
    "    \n",
    "    return box_coords\n",
    "\n",
    "def draw_boxes(image, boxes, classes, thickness=4):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for i in range(len(boxes)):\n",
    "        bot, left, top, right = boxes[i, ...]\n",
    "        class_id = int(classes[i])\n",
    "        color = COLOR_LIST[class_id]\n",
    "        draw.line([(left, top), (left, bot), (right, bot), (right, top), (left, top)], width=thickness, fill=color)\n",
    "        \n",
    "def load_graph(graph_file):\n",
    "    \"\"\"Loads a frozen inference graph\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "\n",
    "# Below we load the graph and extract the relevant tensors using get_tensor_by_name. \n",
    "# These tensors reflect the input and outputs of the graph, or least the ones we care about for detecting objects.\n",
    "\n",
    "# detection_graph = load_graph(SSD_GRAPH_FILE)\n",
    "# detection_graph = load_graph(RFCN_GRAPH_FILE)\n",
    "#detection_graph = load_graph(SSD_INCEPTION_GRAPH_FILE)\n",
    "detection_graph = load_graph(FASTER_RCNN_GRAPH_FILE)\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "# Run detection and classification on a sample image.\n",
    "#image = Image.open('./assets/sample1.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete ...\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "\n",
    "# Run detection and classification on a folder of images.\n",
    "import os\n",
    "import glob\n",
    "\n",
    "path_r = '/Users/cpgrant/Education/Udacity/CarND-Capstone-Team/CarND-Data/data-oliver-171015/real/'\n",
    "image_filter = '*.jpg'\n",
    "class_filter = 10.\n",
    "\n",
    "# Please note the image extension and match to\n",
    "\n",
    "for img_file in glob.glob(os.path.join(path_r, image_filter)):\n",
    "    head, tail = os.path.split(img_file)\n",
    "    \n",
    "    image = Image.open(img_file)\n",
    "    image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "    \n",
    "    with tf.Session(graph=detection_graph) as sess: \n",
    "        \n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], feed_dict={image_tensor: image_np})\n",
    "\n",
    "        # Remove unnecessary dimensions\n",
    "        boxes = np.squeeze(boxes)\n",
    "        scores = np.squeeze(scores)\n",
    "        classes = np.squeeze(classes)\n",
    "\n",
    "        confidence_cutoff = 0.8\n",
    "        # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "        \n",
    "        # The current box coordinates are normalized to a range between 0 and 1.\n",
    "        # This converts the coordinates actual location on the image.\n",
    "        width, height = image.size\n",
    "        \n",
    "        if  len(boxes) >= 1:\n",
    "            box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "            # Each class with be represented by a differently colored box\n",
    "            # draw_boxes(image, box_coords, classes)\n",
    "            \n",
    "            ### For all objects (Consider only getting 1 traffic light if possible)\n",
    "            for i in range(len(boxes)):\n",
    "                \n",
    "                # Filter for traffic lights \n",
    "                if classes.item(i) == class_filter:\n",
    "                    \n",
    "                    # Crop object from image \n",
    "                    b = box_coords[i]\n",
    "                    area = (b[1], b[0], b[3], b[2])\n",
    "                    crop_box = image.crop(area)\n",
    "                    \n",
    "                    # Save image\n",
    "                    path_w = '/Users/cpgrant/Education/Udacity/CarND-Capstone-Team/CarND-Data/data-tl-171015/real/'\n",
    "                    crop_box.save(path_w + str(i) + '-' + tail)\n",
    "            \n",
    "print('Processing complete ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "\n",
    "## Timing Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The model zoo comes with a variety of models, each its benefits and costs. \n",
    "# Below you'll time some of these models. \n",
    "# The general tradeoff being sacrificing model accuracy for seconds per frame (SPF).\n",
    "\n",
    "def time_detection(sess, img_height, img_width, runs=10):\n",
    "    image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
    "    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "    # warmup\n",
    "    gen_image = np.uint8(np.random.randn(1, img_height, img_width, 3))\n",
    "    sess.run([detection_boxes, detection_scores, detection_classes], feed_dict={image_tensor: gen_image})\n",
    "    \n",
    "    times = np.zeros(runs)\n",
    "    for i in range(runs):\n",
    "        t0 = time.time()\n",
    "        sess.run([detection_boxes, detection_scores, detection_classes], feed_dict={image_tensor: image_np})\n",
    "        t1 = time.time()\n",
    "        times[i] = (t1 - t0) * 1000\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    times = time_detection(sess, 600, 1000, runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "plt.title(\"Object Detection Timings\")\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.style.use('fivethirtyeight')\n",
    "bp = ax.boxplot(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Model Tradeoffs\n",
    "# Download a few models from the model zoo and compare the timings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Exploration\n",
    "\n",
    "# Some ideas to take things further:\n",
    "\n",
    "#    Finetune the model on a new dataset more relevant to autonomous vehicles. Instead of loading the frozen inference graph you'll load the checkpoint.\n",
    "#    Optimize the model and get the FPS as low as possible.\n",
    "#    Build your own detector. There are several base model pretrained on ImageNet you can choose from. Keras is probably the quickest way to get setup in this regard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
